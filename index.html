<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> DanceCam </title> <meta name="author" content="DanceCam team"> <meta name="description" content="The DanceCam website "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/dancecam-half.png?f1827b3a4ec75b3424d017163689d00f"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://dancecam.github.io/"> <script src="/assets/js/theme.js?a5ca4084d3b81624bcfa01156dae2b8e"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/"> <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/about/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/news/">news </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/people/">people </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <div class="profile"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/dancecam_logo-480.webp 480w,/assets/img/dancecam_logo-800.webp 800w,/assets/img/dancecam_logo-1400.webp 1400w," sizes="(min-width: 1280px) 615.0px, (min-width: 576px) 30vw, 95vw" type="image/webp"> <img src="/assets/img/dancecam_logo.png?181a78d2f4f3f069fef48bbdfb5f0842" class="img-fluid rounded" width="100%" height="auto" alt="dancecam_logo.png" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <p class="desc"></p> </header> <article> <div class="clearfix"> <h1 style="text-align: center; margin-top: 2rem;">Atmospheric turbulence mitigation in wide-field astronomical images with short-exposure video streams</h1> <h4 id="spencer-bialek--12-emmanuel-bertin--23-sébastien-fabbro--14-hervé-bouy--56-jean-pierre-rivet--7-olivier-lai--7-jean-charles-cuillandre--3"> <a href="mailto:sbialek@uvic.ca">Spencer Bialek</a><a href="https://orcid.org/0009-0007-4388-2508" rel="external nofollow noopener" target="_blank"><sup> <img src="https://orcid.org/assets/vectors/orcid.logo.icon.svg" class="orcid" alt="ORCID iD icon"></sup></a> <sup>1,2</sup>, Emmanuel Bertin<a href="https://orcid.org/0000-0002-3602-3664" rel="external nofollow noopener" target="_blank"><sup> <img src="https://orcid.org/assets/vectors/orcid.logo.icon.svg" class="orcid" alt="ORCID iD icon"></sup></a> <sup>2,3</sup>, Sébastien Fabbro<a href="https://orcid.org/0000-0003-2239-7988" rel="external nofollow noopener" target="_blank"><sup> <img src="https://orcid.org/assets/vectors/orcid.logo.icon.svg" class="orcid" alt="ORCID iD icon"> </sup></a><sup>1,4</sup>, Hervé Bouy<a href="https://orcid.org/0000-0002-7084-487X" rel="external nofollow noopener" target="_blank"><sup> <img src="https://orcid.org/assets/vectors/orcid.logo.icon.svg" class="orcid" alt="ORCID iD icon"></sup></a> <sup>5,6</sup>, Jean-Pierre Rivet<a href="https://orcid.org/0000-0002-0289-5851" rel="external nofollow noopener" target="_blank"><sup> <img src="https://orcid.org/assets/vectors/orcid.logo.icon.svg" class="orcid" alt="ORCID iD icon"></sup></a> <sup>7</sup>, Olivier Lai<a href="https://orcid.org/0000-0001-5656-7346" rel="external nofollow noopener" target="_blank"><sup> <img src="https://orcid.org/assets/vectors/orcid.logo.icon.svg" class="orcid" alt="ORCID iD icon"></sup></a> <sup>7</sup>, Jean-Charles Cuillandre<a href="https://orcid.org/0000-0002-3263-8645" rel="external nofollow noopener" target="_blank"><sup> <img src="https://orcid.org/assets/vectors/orcid.logo.icon.svg" class="orcid" alt="ORCID iD icon"></sup></a> <sup>3</sup> </h4> <p style="font-size: 0.8em; margin-top: 2em;"> <sup>1</sup>Department of Physics and Astronomy, University of Victoria, Victoria, BC, V8W 3P2, Canada<br> <sup>2</sup>Canada–France–Hawaii Telescope, Kamuela, HI 96743, USA<br> <sup>3</sup>AIM, CEA, CNRS, Université Paris-Saclay, Université Paris Cité, F-91191 Gif-sur-Yvette, France<br> <sup>4</sup>National Research Council Herzberg Astronomy and Astrophysics, Victoria, BC, Canada<br> <sup>5</sup>Laboratoire d’Astrophysique de Bordeaux, CNRS and Université de Bordeaux, Allée Geoffroy St. Hilaire, 33165 Pessac, France<br> <sup>6</sup>Institut Universitaire de France<br> <sup>7</sup>Université Côte d’Azur, Observatoire de la Côte d’Azur, CNRS, Laboratoire J.–L. Lagrange, F-06304 Nice Cedex 4, France<br> </p> <div style="text-align:center;"> <a href="https://doi.org/10.1093/mnras/stae1018" rel="external nofollow noopener" target="_blank"><div class="bigbadge" style="margin: auto;"> <img src="/assets/img/icon-pdf.svg" width="30px"> Full paper </div></a> <a href="https://academic.oup.com/Citation/Download?resourceId=7654005&amp;resourceType=3&amp;citationFormat=2" rel="external nofollow noopener" target="_blank"><div class="bigbadge" style="margin: auto;"><img src="/assets/img/icon-bibtex.svg" width="64px"></div></a> </div> <div style="margin-left: auto; margin-right:auto; margin-top: 2em; max-width: 800px"> <h3 style="text-align: center;">Abstract</h3> <p style="font-style: italic; text-align: justify;">We introduce a novel technique to mitigate the adverse effects of atmospheric turbulence on astronomical imaging. Utilizing a video-to-image neural network trained on simulated data, our method processes a sliding sequence of short-exposure (∼0.2s) stellar field images to reconstruct an image devoid of both turbulence and noise. We demonstrate the method with simulated and observed stellar fields, and show that the brief exposure sequence allows the network to accurately associate speckles to their originating stars and effectively disentangle light from adjacent sources across a range of <a href="https://en.wikipedia.org/wiki/Astronomical_seeing" rel="external nofollow noopener" target="_blank">seeing</a> conditions, all while preserving flux to a lower signal-to-noise ratio than an average stack. This approach results in a marked improvement in angular resolution without compromising the <a href="https://en.wikipedia.org/wiki/Astrometry" rel="external nofollow noopener" target="_blank">astrometric</a> stability of the final image.</p> </div> <h3 id="the-problem">The problem</h3> <p>Atmospheric turbulence causes the wavefront of incoming light to be dynamically distorted. As a result, long exposures taken from ground instruments lead to the degradation of images and the overall effect can be framed as a blurring operation. With small telescopes (aperture ≈ 3 − 4 times the <a href="https://en.wikipedia.org/wiki/Fried_parameter" rel="external nofollow noopener" target="_blank">Fried parameter</a>), the tilt component of the wavefront distortions outweighs the sum of all other contributions, and the effect of turbulence is prominently random image motions on the focal plane.</p> <video autoplay="" controls="" loop="" width="100%" style="display:block; margin: 2rem auto 0rem auto;"> <source src="/assets/video/tycho.mp4" type="video/mp4"></source> </video> <div class="caption"> Example of a video sequence at 20 frames per second, showing the dynamical effects of atmospheric turbulence on astronomical observations carried out from the ground on a small telescope (14"). The Moon crater close the center is Tycho. The pixel scale is 0.29". </div> <p><img src="/assets/img/tycho_average.gif" width="100%" style="margin-bottom: -0.7rem; max-width:none !important;"></p> <div class="caption"> 3 second average of the video above, showing the blurring effect of long exposures. </div> <p>As the video above shows, fast imaging provides much sharper images than long exposures, but the high-resolution information is scrambled by turbulence. To compensate the effects of atmospheric turbulence over wide fields, one could imagine correcting those apparent motions by adjusting a “rubber” focal plane model (<a href="https://ui.adsabs.harvard.edu/abs/2000PASP..112..768K/abstract" rel="external nofollow noopener" target="_blank">Kaiser et al. 2000</a>), which consists of a distorted virtual pixel grid.</p> <p>For deep sky observations, which are our primary goal, the deformations of this virtual grid would have to be continuously controlled by a number of guide stars over its surface. However this process is complex to set up in practice, e.g., changing conditions require dynamic adaptation of the statistical model, and the number of individual stars suitable for guiding is often insufficient. Instead, we propose to leverage the power of Deep Learning to directly reconstruct virtual exposures from video sequences.</p> <h3 id="the-method">The method</h3> <p>The cornerstone of our proposed method is the application of the Residual U-Net, a variant of the traditional U-Net neural network architecture known for its proficiency in semantic segmentation and image reconstruction tasks. A set of simulated short-exposure video streams of stellar fields – with turbulence and noise – along with their corresponding ground truth frames – with no turbulence or noise – is used to train the model. Instead of a single output, the model additionally has outputs from each stage in the decoder which are compared to downsampled versions of the ground truth using a weighted mean-squared error (MSE) loss function. Once trained, either a simulated or real video stream can be used as input and only a single (not downsampled) inferred image is retrieved.</p> <p><img class="repo-img-light img-full" src="/assets/img/dancecam_unet.png" width="100%"> <img class="repo-img-dark img-full" src="/assets/img/dancecam_unet-dark.png" width="100%"></p> <div class="caption"> The DanceCam Residual U-Net architecture. </div> <div class="content-section"> <div class="text-content" style="padding:0rem;"> <p>Training of the neural network is done purely on image simulations. We decompose the atmosphere into discrete layers which perturb the wavefront of the light from each star as it passes through. The entire simulation pipeline is written with <a href="https://pytorch.org" rel="external nofollow noopener" target="_blank">PyTorch</a> so that GPUs could be maximally utilized with Fast Fourier Transforms. This results in the capability to render ∼150,000 point sources per second, which is a couple orders of magnitude faster than other similar implementations.</p> <p>We generated training datasets containing 40,000 6- and 12-second video sequences; 12 seconds was eventually chosen as a compromise between GPU memory constraints and collecting enough information about the turbulence and faint stars. Each frame matches the properties of the wide-field camera at the C2PU Omicron Telescope. Along with each video sequence, we generated the corresponding ground truth frame in which we disabled contributions from the atmosphere and any sources of noise in our simulation pipeline.</p> </div> <div class="img-right" style="margin-left: 1rem; margin-right: 0rem; padding: 0rem;"> <img class="repo-img-light" src="/assets/img/phasescreenlayers.png" data-zoomable="" style="max-width: 320px;"> <img class="repo-img-dark" src="/assets/img/phasescreenlayers-dark.png" data-zoomable="" style="max-width: 320px;"> <div class="caption"> An example of the phase screens used in the simulation pipeline. </div> </div> </div> <video autoplay="" controls="" loop="" width="100%" style="display:block; margin: 2rem auto 0rem auto;"> <source src="/assets/video/tycho_sim.mp4" type="video/mp4"></source> </video> <div class="caption"> Example of a simulated video sequence of a deep star field observed under conditions roughly comparable to those of the Moon above. </div> <h3 id="results">Results</h3> <h4 id="synthetic-data">Synthetic data</h4> <p>Visually, the proposed method does an excellent job at taking in a short sequence of turbulent images and producing clear, sharp, noise- and turbulence-mitigated images.</p> <video autoplay="" controls="" loop="" width="100%" style="max-width: 1200px; display:block; margin: 2rem auto 0rem auto;"> <source src="/assets/video/videoexample12s_simrandomfield_0.7arcsec.mp4" type="video/mp4"></source> </video> <video autoplay="" controls="" loop="" width="100%" style="max-width: 1200px; display:block; margin: 0.5rem auto 0rem auto;"> <source src="/assets/video/videoexample12s_simrandomfield_1.4arcsec.mp4" type="video/mp4"></source> </video> <div class="caption"> Two examples highlighting the ability of the proposed method to remove the effects of atmospheric turbulence and produce a sharp, clear image. Sequences of random stellar fields were simulated with (top) 0.7" seeing and (bottom) 1.4" seeing, and, from left to right: the ground truth, the video sequence, the temporally averaged sequence, and the sequence of inferred frames. </div> <p>A series of quality assurance tests were made to validate the image reconstructions made by the U-Net. Hundreds of 30-second simulated observations of random stellar fields, with varying seeing conditions, were created and two images were made for each example: a stack made from the U-Net inferred images and a simple averaged stack of the raw frames.</p> <p><img class="repo-img-light img-full" src="/assets/img/sim_m92_measurements.png" width="100%"> <img class="repo-img-dark img-full" src="/assets/img/sim_m92_measurements-dark.png" width="100%"></p> <div class="caption"> 10-pixel aperture <a href="https://en.wikipedia.org/wiki/Photometry_(astronomy)#Magnitudes_and_colour_indices" rel="external nofollow noopener" target="_blank">magnitudes</a> (left panel), source sizes (defined as D50, the diameter of the circle within which 50% of the light from a star is contained, middle panel) and centroid coordinates (right panels). Shown here are the residuals of those metrics for the inferred stack (orange disks) and simple averaged stack (blue triangles) when compared to the matching stars in the ground truth frames as a function of magnitude, along with their binned means and standard deviations (shown as error bars) – where the black and grey lines correspond to the inferred and averaged stack values, respectively. Also shown are the computed means for “bad seeing" and “good seeing" subsets of the data (&gt; 1.2" and &lt; 0.7", respectively). </div> <h4 id="real-data">Real data</h4> <p>We directly applied the neural network trained from synthetic data to real video sequences taken at the prime focus of the C2PU 1m “Omicron” telescope (Calern observatory, France) <em>without transfer learning or domain adaptation</em>. As the illustrations below show, the proposed method is directly applicable to real video sequences, and provides a marked improvement over regular image stacking, both qualitatively and quantitatively. The inferred frames exhibited a typical 2.5\(\times\) reduction in D50 measurements, and more than 30% more stars are identified relative to the averaged frame.</p> <video autoplay="" controls="" loop="" width="100%" style="max-width: 1200px; display:block; margin: 2rem auto 0rem auto;"> <source src="/assets/video/videoexample12s_M92.mp4" type="video/mp4"></source> </video> <div class="caption"> The C2PU telescope was used to obtain a video stream at 5.25 frames/sec of the globular cluster M92 in the red filter, which was decomposed into 256x256 video tiles of 12s and fetched into our neural network model. From left to right: a single frame from the video (zoomed in), the video sequence, the temporally averaged sequence, and the sequence of inferred frames. Note the intermittently turbulent ground layer that can cause all the stars in the field to move in lockstep by up to a couple arcseconds, an effect which is not currently accounted for in the simulations. </div> <p><img class="repo-img-light img-full" src="/assets/img/real_m92_stats.png" width="100%" style="margin-top: 2rem; max-width: 800px; margin-left: auto; margin-right: auto"> <img class="repo-img-dark img-full" src="/assets/img/real_m92_stats-dark.png" width="100%" style="margin-top: 2rem; max-width: 800px; margin-left: auto; margin-right: auto;"></p> <div class="caption" style="max-width: 800px; margin-left: auto; margin-right: auto;"> Source sizes (D50, left panel) and stellar detection completeness (for stars from the Gaia DR3 catalog, at a 2% FPR, right panel) as a function of Gaia magnitude, for the real M92 observations. The orange symbols are for a 30s stack derived of the inferred virtual exposures, while the blue symbols are for the simple averaged image stack. </div> <h2 id="conclusion">Conclusion</h2> <p>Our method, trained on simulated observations, is adept at inferring a turbulence- and noise-free image from a sequence of short-exposure observations of a stellar field, effectively associating speckles with their source star and disentangling light from proximate sources, while conserving flux. However, it is important to acknowledge that further development and refinement are necessary for this approach, particularly in recovering fainter sources in low stellar density environments, improving astrometric precision, and reconstructing the images of extended objects such as galaxies.</p> <p>Checkout <a href="https://doi.org/10.1093/mnras/stae1018" rel="external nofollow noopener" target="_blank">the full paper</a>.</p> </div> </article> </div> </div> <footer class="sticky-bottom mt-5" role="contentinfo"> <div class="container"> © Copyright 2024 DanceCam team. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?b7816bd189846d29eded8745f9c4cf77"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>